{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics.nn.tasks import DetectionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du modèle .best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des classes et du choix de la classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe_choisie = \"2x4_Jaune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"1x2_Blanc\",\n",
    "    \"1x2_Bleu\",\n",
    "    \"1x2_Jaune\",\n",
    "    \"1x2_Marron\",\n",
    "    \"1x2_Noir\",\n",
    "    \"1x2_Rouge\",\n",
    "    \"1x2_Vert clear\",\n",
    "    \"1x2_Vert dark\",\n",
    "    \"1x4_Blanc\",\n",
    "    \"1x4_Jaune\",\n",
    "    \"1x4_Noir\",\n",
    "    \"1x4_Rouge\",\n",
    "    \"1x4_Vert clear\",\n",
    "    \"1x4_Vert dark\",\n",
    "    \"2x2_Blanc\",\n",
    "    \"2x2_Bleu\",\n",
    "    \"2x2_Jaune\",\n",
    "    \"2x2_Marron\",\n",
    "    \"2x2_Rouge\",\n",
    "    \"2x2_Vert clear\",\n",
    "    \"2x2_Vert dark\",\n",
    "    \"2x4_Blanc\",\n",
    "    \"2x4_Bleu\",\n",
    "    \"2x4_Jaune\",\n",
    "    \"2x4_Rouge\",\n",
    "    \"2x4_Vert dark\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de traitement pour la video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration initiale\n",
    "VIDEO_SOURCE = \"/home/dim/clone_repo/BrickSearch/data/raw/videos/lego_video_test.mp4\"\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "MODEL_NAME = \"y12x_100_640_3000_16_0_065\"\n",
    "MODEL_PATH = f\"/home/dim/clone_repo/BrickSearch/outputs/output_models/{MODEL_NAME}/weights/best.pt\"\n",
    "OUTPUT_FILE = \"/home/dim/clone_repo/BrickSearch/outputs/video_annotees/result_\" + VIDEO_SOURCE.replace(\"/home/dim/clone_repo/BrickSearch/data/raw/videos/\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    \"\"\"\n",
    "    Initialise les modèles .\n",
    "\n",
    "    Returns:\n",
    "        yolo_model entrainé lego: \n",
    "    \"\"\"\n",
    "# Chargement du modèle sur le GPU \n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cuda'))\n",
    "\n",
    "    model = checkpoint['model']\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_video_capture_and_writer():\n",
    "    \"\"\"\n",
    "    Initialise la capture vidéo et l'écriture vidéo.\n",
    "\n",
    "    Returns:\n",
    "        cap: Objet de capture vidéo.\n",
    "        video_writer: Objet d'écriture vidéo.\n",
    "    \"\"\"    \n",
    "    cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # type: ignore\n",
    "    video_writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "    return cap, video_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(model, frame):\n",
    "    \"\"\"\n",
    "    Détecte les objets dans un frame avec YOLO.\n",
    "\n",
    "    Args:\n",
    "        model: Modèle YOLO.\n",
    "        frame: Frame vidéo.\n",
    "\n",
    "    Returns:\n",
    "        detections_list: Liste des détections avec leurs coordonnées, confiance et classe.\n",
    "    \"\"\"\n",
    "    results = model(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)[0]\n",
    "    detections_list = []\n",
    "    for det in results.boxes.data:\n",
    "        x1, y1, x2, y2, conf, cls = det.cpu().numpy()\n",
    "        if int(cls) == 0:\n",
    "            largeur = x2 - x1\n",
    "            hauteur = y2 - y1\n",
    "            detections_list.append([[x1, y1, largeur, hauteur], conf, int(cls)])\n",
    "    return detections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracks(tracker, detections_list, frame):\n",
    "    \"\"\"\n",
    "    Met à jour les pistes avec DeepSort.\n",
    "\n",
    "    Args:\n",
    "        tracker: Tracker DeepSort.\n",
    "        detections_list: Liste des détections.\n",
    "        frame: Frame vidéo.\n",
    "\n",
    "    Returns:\n",
    "        track_boxes: Liste des boîtes de suivi.\n",
    "        track_ids: Liste des IDs de suivi.\n",
    "    \"\"\"\n",
    "    tracks = tracker.update_tracks(detections_list, frame=frame)\n",
    "    track_boxes = []\n",
    "    track_ids = []\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()\n",
    "        track_boxes.append(bbox)\n",
    "        track_ids.append(track_id)\n",
    "    return track_boxes, track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame(frame, track_boxes, track_ids, annotator):\n",
    "    \"\"\"\n",
    "    Annote les boîtes de suivi sur le frame.\n",
    "\n",
    "    Args:\n",
    "        frame: Frame vidéo.\n",
    "        track_boxes: Liste des boîtes de suivi.\n",
    "        track_ids: Liste des IDs de suivi.\n",
    "        annotator: Annotateur pour les boîtes de détection.\n",
    "\n",
    "    Returns:\n",
    "        annotated_frame: Frame annoté.\n",
    "    \"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    if track_boxes:\n",
    "        detections_obj = Detections(\n",
    "            xyxy=np.array(track_boxes),\n",
    "            class_id=np.array(track_ids, dtype=int)\n",
    "        )\n",
    "        annotated_frame = annotator.annotate(scene=annotated_frame, detections=detections_obj)\n",
    "        for bbox, tid in zip(track_boxes, track_ids):\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cv2.putText(annotated_frame, f\"ID {tid}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video():\n",
    "    \"\"\"\n",
    "    Traite la vidéo en détectant, suivant et annotant les objets et les poses.\n",
    "    \"\"\"\n",
    "    # yolo_model, pose_landmarker = initialize_models()\n",
    "    # tracker = initialize_tracker()\n",
    "    cap, video_writer = initialize_video_capture_and_writer()\n",
    "    # annotator = BoxAnnotator(thickness=2)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # detections_list = detect_objects(yolo_model, frame)\n",
    "        # track_boxes, track_ids = update_tracks(tracker, detections_list, frame)\n",
    "        # annotated_frame = annotate_frame(frame, track_boxes, track_ids, annotator)\n",
    "\n",
    "        timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "        roi_timestamp = timestamp_ms\n",
    "\n",
    "        # for bbox, tid in zip(track_boxes, track_ids):\n",
    "        #     x1, y1, x2, y2 = map(int, bbox)\n",
    "        #     if x1 < 0 or y1 < 0 or x2 > frame.shape[1] or y2 > frame.shape[0]:\n",
    "        #         continue\n",
    "\n",
    "        #     roi = annotated_frame[y1:y2, x1:x2]\n",
    "\n",
    "        #     if roi.size == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     pose_landmarker_result = detect_poses(pose_landmarker, roi, roi_timestamp)\n",
    "        #     roi_timestamp += 1\n",
    "        #     roi = draw_poses(roi, pose_landmarker_result, tid)\n",
    "        #     annotated_frame[y1:y2, x1:x2] = roi\n",
    "\n",
    "\n",
    "        # Définition des paramètres de la fenêtre d'affichage\n",
    "        WINDOW_NAME = 'YOLOv8 Detection'\n",
    "        WINDOW_WIDTH = 800   # Nouvelle largeur souhaitée\n",
    "        WINDOW_HEIGHT = 600  # Nouvelle hauteur souhaitée\n",
    "        WINDOW_POS_X = 100   # Position horizontale sur l'écran\n",
    "        WINDOW_POS_Y = 100   # Position verticale sur l'écran\n",
    "\n",
    "        # Création et configuration de la fenêtre\n",
    "        cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(WINDOW_NAME, WINDOW_WIDTH, WINDOW_HEIGHT)\n",
    "        cv2.moveWindow(WINDOW_NAME, WINDOW_POS_X, WINDOW_POS_Y)\n",
    "\n",
    "        \n",
    "        cv2.imshow(\"YOLOv8 Detection\", frame)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
